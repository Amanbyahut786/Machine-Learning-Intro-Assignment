{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Intro | Assignment"
      ],
      "metadata": {
        "id": "e8ye3ryNaD1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1: Explain the differences between AI, ML, Deep Learning (DL), and Data Science (DS).\n"
      ],
      "metadata": {
        "id": "30CzNPiiaQw5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d8b7c11"
      },
      "source": [
        "*  the differences between AI, ML, Deep Learning (DL), and Data Science (DS):\n",
        "\n",
        "*   **Artificial Intelligence (AI)**: This is the broadest concept. AI is the simulation of human intelligence in machines that are programmed to think and learn like humans. It's about creating intelligent agents that can reason, plan, perceive, learn, and act.\n",
        "\n",
        "*   **Machine Learning (ML)**: ML is a subset of AI. It's a field of study that gives computers the ability to learn without being explicitly programmed. Instead of writing code for every possible scenario, ML algorithms learn from data and make predictions or decisions based on that learning.\n",
        "\n",
        "*   **Deep Learning (DL)**: DL is a subset of ML. It's inspired by the structure and function of the human brain (artificial neural networks). Deep learning models use multiple layers of these networks to process data and learn complex patterns. DL is particularly effective for tasks involving images, speech, and natural language.\n",
        "\n",
        "*   **Data Science (DS)**: Data Science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data in various forms, both structured and unstructured. It encompasses various techniques and fields, including statistics, ML, data analysis, and domain expertise, to understand and interpret complex data.\n",
        "\n",
        "**Think of it like this:**\n",
        "\n",
        "*   **AI** is the overarching goal: making machines intelligent.\n",
        "*   **ML** is one way to achieve AI: by enabling machines to learn from data.\n",
        "*   **DL** is a specific technique within ML: using deep neural networks for complex pattern recognition.\n",
        "*   **Data Science** is a field that *uses* AI and ML (among other techniques) to analyze data and extract valuable insights."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 2: What are the types of machine learning? Describe each with one real-world example"
      ],
      "metadata": {
        "id": "XB7RyyRla56H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c30e81a2"
      },
      "source": [
        "* There are three main types of machine learning:\n",
        "\n",
        "1.  **Supervised Learning**: In supervised learning, the model is trained on a labeled dataset, meaning the data includes both the input features and the desired output. The goal is for the model to learn a mapping from the input to the output so it can predict the output for new, unseen data.\n",
        "    *   **Real-world example**: **Spam email detection**. The model is trained on a dataset of emails labeled as \"spam\" or \"not spam\". It learns to identify patterns in the email content (like specific words or phrases) that indicate whether an email is spam and can then predict if a new email is spam or not.\n",
        "\n",
        "2.  **Unsupervised Learning**: In unsupervised learning, the model is trained on an unlabeled dataset. The goal is to find hidden patterns or structures in the data without any predefined output.\n",
        "    *   **Real-world example**: **Customer segmentation**. A company might use unsupervised learning to group its customers based on their purchasing behavior, demographics, or other attributes. This helps them understand different customer groups and tailor marketing strategies accordingly.\n",
        "\n",
        "3.  **Reinforcement Learning**: In reinforcement learning, an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions, and its goal is to learn a policy that maximizes the cumulative reward over time.\n",
        "    *   **Real-world example**: **Training a robot to walk**. The robot is the agent, and the environment is the physical world. The robot receives rewards for moving forward and penalties for falling. Through trial and error, the robot learns the sequence of movements (actions) that allow it to walk successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3: Define overfitting, underfitting, and the bias-variance tradeoff in machine learning.\n"
      ],
      "metadata": {
        "id": "X4uTYeexbQ0X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d69e6d5"
      },
      "source": [
        "* Here are the definitions for overfitting, underfitting, and the bias-variance tradeoff:\n",
        "\n",
        "*   **Overfitting**: Overfitting occurs when a machine learning model learns the training data too well, including the noise and outliers. This results in a model that performs very well on the training data but poorly on new, unseen data because it has essentially memorized the training examples instead of learning the underlying patterns. An overfitted model is too complex for the data.\n",
        "\n",
        "*   **Underfitting**: Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the training data. The model doesn't learn the relationships between the features and the target variable effectively, resulting in poor performance on both the training data and new data. An underfitted model is not complex enough for the data.\n",
        "\n",
        "*   **Bias-Variance Tradeoff**: This is a fundamental concept in machine learning. It refers to the conflict between minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training data:\n",
        "    *   **Bias**: Bias is the error introduced by approximating a real-world problem, which may be complicated, by a simplified model. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
        "    *   **Variance**: Variance is the error introduced from the sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).\n",
        "\n",
        "*  The **bias-variance tradeoff** is the challenge of simultaneously minimizing both bias and variance. Decreasing bias often increases variance, and vice versa. The goal is to find a balance that minimizes the total error and allows the model to generalize well to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: What are outliers in a dataset, and list three common techniques for handling them"
      ],
      "metadata": {
        "id": "64xfItMIcR_P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2f83f3"
      },
      "source": [
        "* Here's an explanation of outliers and common techniques for handling them:\n",
        "\n",
        "*   **Outliers**: Outliers are data points that are significantly different from other observations in a dataset. They are extreme values that lie far away from the majority of the data. Outliers can occur due to various reasons, such as measurement errors, data entry mistakes, or genuinely rare events. They can negatively impact the performance of machine learning models by distorting statistical measures and model training.\n",
        "\n",
        "*   **Common techniques for handling outliers**:\n",
        "\n",
        "    1.  **Removal (Deletion)**: This is the simplest technique, where outlier data points are simply removed from the dataset. This is often done when the outlier is suspected to be due to an error and doesn't represent the true data distribution. However, removing outliers can lead to loss of valuable information and should be done cautiously, especially in small datasets.\n",
        "\n",
        "    2.  **Transformation**: This involves applying mathematical transformations to the data to reduce the impact of outliers. Common transformations include:\n",
        "        *   **Log transformation**: Useful for skewed data, as it compresses larger values and expands smaller values.\n",
        "        *   **Square root transformation**: Similar to log transformation, it reduces the spread of the data.\n",
        "        *   **Box-Cox transformation**: A family of power transformations that can be applied to make the data more normally distributed.\n",
        "\n",
        "    3.  **Imputation**: Instead of removing outliers, you can replace them with a more representative value. This can be done using various imputation techniques, such as:\n",
        "        *   **Mean, median, or mode imputation**: Replacing the outlier with the mean, median, or mode of the non-outlier data. The median is often preferred as it is less affected by extreme values.\n",
        "        *   **Model-based imputation**: Using a machine learning model to predict the outlier value based on other features in the dataset.\n",
        "        *   **Winsorizing**: Capping the outliers at a certain percentile (e.g., replacing values above the 95th percentile with the value at the 95th percentile).\n",
        "\n",
        "* The choice of technique depends on the nature of the data, the suspected cause of the outliers, and the specific machine learning model being used. It's often a good practice to analyze the impact of different techniques on model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5: Explain the process of handling missing values and mention one imputation technique for numerical and one for categorical data.\n"
      ],
      "metadata": {
        "id": "gGpiq-gucpof"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5465d483"
      },
      "source": [
        "* Here's an explanation of handling missing values and some imputation techniques:\n",
        "\n",
        "* **Handling Missing Values**\n",
        "\n",
        "* Missing values, also known as NaNs (Not a Number), are common in real-world datasets and can significantly impact the performance of machine learning models. Handling missing values is a crucial step in data preprocessing. The process typically involves:\n",
        "\n",
        "1.  **Identification**: Detecting where the missing values are located in the dataset. This can be done by checking for null or missing values in each column.\n",
        "2.  **Analysis**: Understanding the pattern and extent of missing values. Are they random, or are they related to other features? The proportion of missing values can also influence the handling strategy.\n",
        "3.  **Handling Strategy**: Deciding how to address the missing values. Common strategies include:\n",
        "    *   **Deletion**: Removing rows or columns with missing values. This is suitable if the number of missing values is small and doesn't lead to significant data loss.\n",
        "    *   **Imputation**: Replacing missing values with estimated values. This is often preferred when deletion would result in a substantial loss of data.\n",
        "    *   **Ignoring**: Some machine learning algorithms can handle missing values internally, so you might choose to leave them as they are.\n",
        "\n",
        "* **Imputation Techniques**\n",
        "\n",
        "* Imputation is the process of replacing missing values with substitute values. The choice of imputation technique depends on the type of data (numerical or categorical) and the nature of the missingness.\n",
        "\n",
        "*   **Imputation Technique for Numerical Data**:\n",
        "\n",
        "    *   **Mean/Median Imputation**: Replacing missing numerical values with the mean or median of the non-missing values in that column. The median is often more robust to outliers.\n",
        "\n",
        "*   **Imputation Technique for Categorical Data**:\n",
        "\n",
        "    *   **Mode Imputation**: Replacing missing categorical values with the mode (most frequent category) of the non-missing values in that column.\n",
        "\n",
        "* Other more advanced imputation techniques exist, such as using k-nearest neighbors (KNN) or model-based imputation, but mean/median and mode imputation are simple and commonly used methods."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 6: Write a Python program that:\n",
        "###● Creates a synthetic imbalanced dataset with make_classification() from sklearn.datasets.\n",
        "###● Prints the class distribution.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "KE9baWg-dMsn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62c39550"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a synthetic imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, # Total number of samples\n",
        "                           n_features=20, # Number of features\n",
        "                           n_informative=2, # Number of informative features\n",
        "                           n_redundant=10, # Number of redundant features\n",
        "                           n_clusters_per_class=1, # Number of clusters per class\n",
        "                           weights=[0.9, 0.1], # The proportions of samples assigned to each class\n",
        "                           flip_y=0.01, # The fraction of samples whose class is randomly assigned\n",
        "                           class_sep=0.8, # The separation between classes\n",
        "                           random_state=1) # Seed for reproducibility\n",
        "\n",
        "# Print the class distribution\n",
        "print(\"Class distribution:\", Counter(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 7: Implement one-hot encoding using pandas for the following list of colors:\n",
        "##['Red', 'Green', 'Blue', 'Green', 'Red']. Print the resulting dataframe.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "IMHdNgX6d8pP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66df05c1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of colors\n",
        "colors = ['Red', 'Green', 'Blue', 'Green', 'Red']\n",
        "\n",
        "# Create a pandas Series from the list\n",
        "colors_series = pd.Series(colors)\n",
        "\n",
        "# Implement one-hot encoding\n",
        "one_hot_encoded_df = pd.get_dummies(colors_series)\n",
        "\n",
        "# Print the resulting dataframe\n",
        "print(one_hot_encoded_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 8: Write a Python script to:\n",
        "###● Generate 1000 samples from a normal distribution.\n",
        "###● Introduce 50 random missing values.\n",
        "###● Fill missing values with the column mean.\n",
        "###● Plot a histogram before and after imputation.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "j0GFVXpKeux-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5505f2d3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate 1000 samples from a normal distribution\n",
        "np.random.seed(42) # for reproducibility\n",
        "data = np.random.normal(loc=0, scale=1, size=1000)\n",
        "data_series = pd.Series(data)\n",
        "\n",
        "# Introduce 50 random missing values\n",
        "missing_indices = np.random.choice(len(data_series), size=50, replace=False)\n",
        "data_with_missing = data_series.copy()\n",
        "data_with_missing[missing_indices] = np.nan\n",
        "\n",
        "# Plot histogram before imputation\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data_with_missing.dropna(), bins=30, edgecolor='black')\n",
        "plt.title('Histogram Before Imputation')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Fill missing values with the column mean\n",
        "mean_value = data_with_missing.mean()\n",
        "data_imputed = data_with_missing.fillna(mean_value)\n",
        "\n",
        "# Plot histogram after imputation\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(data_imputed, bins=30, edgecolor='black')\n",
        "plt.title('Histogram After Mean Imputation')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 9: Implement Min-Max scaling on the following list of numbers [2, 5, 10, 15,20] using sklearn.preprocessing.MinMaxScaler. Print the scaled array.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "NHW3Sb5SfhwP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb4f4202"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# List of numbers\n",
        "data = np.array([2, 5, 10, 15, 20]).reshape(-1, 1) # Reshape for MinMaxScaler\n",
        "\n",
        "# Implement Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Print the scaled array\n",
        "print(scaled_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 10: You are working as a data scientist for a retail company. You receive a customer\n",
        "##transaction dataset that contains:\n",
        "###● Missing ages,\n",
        "###● Outliers in transaction amount,\n",
        "###● A highly imbalanced target (fraud vs. non-fraud),\n",
        "###● Categorical variables like payment method.\n",
        "###Explain the step-by-step data preparation plan you’d follow before training a machine learning model. Include how you’d address missing data, outliers, imbalance, and encoding.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "BQhTB9AvgIoW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f69b883"
      },
      "source": [
        "# Task\n",
        "Explain a step-by-step data preparation plan for a customer transaction dataset with missing ages, outliers in transaction amount, a highly imbalanced target (fraud vs. non-fraud), and categorical variables like payment method, before training a machine learning model. Include how to address missing data, outliers, imbalance, and encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d3ea8f3"
      },
      "source": [
        "## Understand the data\n",
        "\n",
        "### Subtask:\n",
        "Load the dataset and get a general understanding of its structure, features, and the extent of the issues mentioned (missing values, outliers, imbalance).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f15afa5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the dataset to understand its structure and identify the issues mentioned in the problem description. Since the dataset is not provided, I will create a synthetic dataset that simulates the characteristics described in the problem to demonstrate the data preparation steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eaab26e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a synthetic dataset simulating the described issues\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "n_samples = 10000\n",
        "# Simulate age with some missing values\n",
        "age = np.random.randint(18, 90, size=n_samples)\n",
        "missing_age_indices = np.random.choice(n_samples, size=int(0.05 * n_samples), replace=False)\n",
        "age[missing_age_indices] = np.nan\n",
        "\n",
        "# Simulate transaction amount with some outliers\n",
        "transaction_amount = np.random.normal(loc=50, scale=20, size=n_samples)\n",
        "outlier_indices = np.random.choice(n_samples, size=int(0.01 * n_samples), replace=False)\n",
        "transaction_amount[outlier_indices] = np.random.uniform(200, 1000, size=int(0.01 * n_samples)) # Introduce outliers\n",
        "\n",
        "# Simulate payment method (categorical)\n",
        "payment_method = np.random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Bank Transfer'], size=n_samples)\n",
        "\n",
        "# Simulate a highly imbalanced target variable (fraud vs. non-fraud)\n",
        "target = np.zeros(n_samples, dtype=int)\n",
        "fraud_indices = np.random.choice(n_samples, size=int(0.02 * n_samples), replace=False)\n",
        "target[fraud_indices] = 1\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Age': age,\n",
        "    'Transaction_Amount': transaction_amount,\n",
        "    'Payment_Method': payment_method,\n",
        "    'Fraud': target\n",
        "})\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "display(df.head())\n",
        "\n",
        "# Get info about the dataset\n",
        "print(\"\\nDataset Info:\")\n",
        "df.info()\n",
        "\n",
        "# Get descriptive statistics of numerical columns\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "display(df.describe())\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "print(\"\\nTarget Variable Distribution:\")\n",
        "display(df['Fraud'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "998fce5c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to introduce missing values in the 'Age' column failed because the column was created with an integer data type, which cannot hold NaN (a float). To fix this, I need to ensure the 'Age' column can accommodate float values before introducing missing data. I will recreate the synthetic dataset with this correction and proceed with the initial data exploration steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hlLMaKwh5qq"
      },
      "source": [
        "# Create a synthetic dataset simulating the described issues\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "n_samples = 10000\n",
        "# Simulate age with some missing values, initialize as float to allow NaN\n",
        "age = np.random.randint(18, 90, size=n_samples).astype(float)\n",
        "missing_age_indices = np.random.choice(n_samples, size=int(0.05 * n_samples), replace=False)\n",
        "age[missing_age_indices] = np.nan\n",
        "\n",
        "# Simulate transaction amount with some outliers\n",
        "transaction_amount = np.random.normal(loc=50, scale=20, size=n_samples)\n",
        "outlier_indices = np.random.choice(n_samples, size=int(0.01 * n_samples), replace=False)\n",
        "transaction_amount[outlier_indices] = np.random.uniform(200, 1000, size=int(0.01 * n_samples)) # Introduce outliers\n",
        "\n",
        "# Simulate payment method (categorical)\n",
        "payment_method = np.random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Bank Transfer'], size=n_samples)\n",
        "\n",
        "# Simulate a highly imbalanced target variable (fraud vs. non-fraud)\n",
        "target = np.zeros(n_samples, dtype=int)\n",
        "fraud_indices = np.random.choice(n_samples, size=int(0.02 * n_samples), replace=False)\n",
        "target[fraud_indices] = 1\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Age': age,\n",
        "    'Transaction_Amount': transaction_amount,\n",
        "    'Payment_Method': payment_method,\n",
        "    'Fraud': target\n",
        "})\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "display(df.head())\n",
        "\n",
        "# Get info about the dataset\n",
        "print(\"\\nDataset Info:\")\n",
        "df.info()\n",
        "\n",
        "# Get descriptive statistics of numerical columns\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "display(df.describe())\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "print(\"\\nTarget Variable Distribution:\")\n",
        "display(df['Fraud'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27a7fee2"
      },
      "source": [
        "## Handle missing values\n",
        "\n",
        "### Subtask:\n",
        "Address the missing ages, likely using an imputation technique such as median imputation since age can have a skewed distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ded8a91"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the median of the 'Age' column and fill the missing values with this median. Then, verify that the missing values have been filled.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da2d2948"
      },
      "source": [
        "# Calculate the median of the 'Age' column\n",
        "median_age = df['Age'].median()\n",
        "print(f\"Median age: {median_age}\")\n",
        "\n",
        "# Fill missing values in the 'Age' column with the median\n",
        "df['Age'].fillna(median_age, inplace=True)\n",
        "\n",
        "# Verify that there are no more missing values in the 'Age' column\n",
        "print(\"\\nMissing values in 'Age' after imputation:\")\n",
        "print(df['Age'].isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59ba237d"
      },
      "source": [
        "## Handle outliers\n",
        "\n",
        "### Subtask:\n",
        "Identify and handle outliers in the transaction amount. Techniques like Winsorizing or transformation could be considered, depending on the distribution of the transaction amounts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4c807bc"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the IQR, define bounds, visualize the distribution, handle outliers using Winsorizing, and visualize again to verify.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bca9c0c5"
      },
      "source": [
        "# Calculate Q1, Q3, and IQR for 'Transaction_Amount'\n",
        "Q1 = df['Transaction_Amount'].quantile(0.25)\n",
        "Q3 = df['Transaction_Amount'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define bounds for outlier detection (using 1.5 * IQR rule for reference)\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Q1 (25th percentile): {Q1}\")\n",
        "print(f\"Q3 (75th percentile): {Q3}\")\n",
        "print(f\"IQR: {IQR}\")\n",
        "print(f\"Lower bound (1.5*IQR rule): {lower_bound}\")\n",
        "print(f\"Upper bound (1.5*IQR rule): {upper_bound}\")\n",
        "\n",
        "# Visualize the distribution of 'Transaction_Amount' before handling outliers\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.boxplot(df['Transaction_Amount'])\n",
        "plt.title('Box Plot of Transaction Amount Before Winsorizing')\n",
        "plt.ylabel('Transaction Amount')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df['Transaction_Amount'], bins=50, edgecolor='black')\n",
        "plt.title('Histogram of Transaction Amount Before Winsorizing')\n",
        "plt.xlabel('Transaction Amount')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Implement Winsorizing by capping at the 99th percentile\n",
        "upper_cap = df['Transaction_Amount'].quantile(0.99)\n",
        "df['Transaction_Amount_Winsorized'] = np.clip(df['Transaction_Amount'], a_min=None, a_max=upper_cap)\n",
        "\n",
        "print(f\"\\nUpper cap for Winsorizing (99th percentile): {upper_cap}\")\n",
        "\n",
        "# Verify that the outliers have been handled by visualizing the distribution after Winsorizing\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.boxplot(df['Transaction_Amount_Winsorized'])\n",
        "plt.title('Box Plot of Transaction Amount After Winsorizing')\n",
        "plt.ylabel('Transaction Amount')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df['Transaction_Amount_Winsorized'], bins=50, edgecolor='black')\n",
        "plt.title('Histogram of Transaction Amount After Winsorizing')\n",
        "plt.xlabel('Transaction Amount')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f3f17cd"
      },
      "source": [
        "## Encode categorical variables\n",
        "\n",
        "### Subtask:\n",
        "Convert categorical variables like payment method into a numerical format suitable for machine learning models, using techniques like one-hot encoding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7777e424"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the 'Payment_Method' categorical column into numerical format using one-hot encoding and update the dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "646f9851"
      },
      "source": [
        "# Convert 'Payment_Method' into one-hot encoded columns\n",
        "payment_method_encoded = pd.get_dummies(df['Payment_Method'], prefix='Payment_Method')\n",
        "\n",
        "# Concatenate the new one-hot encoded columns with the original DataFrame and drop the original 'Payment_Method' column\n",
        "df = pd.concat([df.drop('Payment_Method', axis=1), payment_method_encoded], axis=1)\n",
        "\n",
        "# Display the first few rows of the updated DataFrame\n",
        "print(\"First 5 rows of the DataFrame after one-hot encoding:\")\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aa3b9ef"
      },
      "source": [
        "## Address data imbalance\n",
        "\n",
        "### Subtask:\n",
        "Handle the highly imbalanced target variable (fraud vs. non-fraud) using techniques like oversampling the minority class (e.g., SMOTE) or undersampling the majority class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aacd9c74"
      },
      "source": [
        "**Reasoning**:\n",
        "Separate features and target, then apply SMOTE to handle the class imbalance and print the resampled class distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7a064ab"
      },
      "source": [
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('Fraud', axis=1)\n",
        "y = df['Fraud']\n",
        "\n",
        "# Instantiate SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Apply SMOTE to the dataset\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Print the class distribution of the resampled target variable\n",
        "print(\"Class distribution after SMOTE:\", Counter(y_resampled))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aea9bdc2"
      },
      "source": [
        "## Feature scaling\n",
        "\n",
        "### Subtask:\n",
        "Apply feature scaling (e.g., StandardScaler or MinMaxScaler) to numerical features, especially if the chosen machine learning model is sensitive to the scale of features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fef3ce2b"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify numerical features, apply StandardScaler, and replace original numerical columns with scaled data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1767ae01"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical columns (excluding one-hot encoded columns)\n",
        "numerical_cols = ['Age', 'Transaction_Amount', 'Transaction_Amount_Winsorized']\n",
        "\n",
        "# Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply StandardScaler to the numerical columns of X_resampled\n",
        "X_resampled[numerical_cols] = scaler.fit_transform(X_resampled[numerical_cols])\n",
        "\n",
        "# Display the first few rows of the scaled X_resampled DataFrame\n",
        "print(\"First 5 rows of the scaled X_resampled DataFrame:\")\n",
        "display(X_resampled.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1bdb29f"
      },
      "source": [
        "## Split data\n",
        "\n",
        "### Subtask:\n",
        "Split the prepared data into training and testing sets to evaluate the model's performance on unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701b5d91"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the resampled data into training and testing sets and print their shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a3f3529"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the resampled data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6790273a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial synthetic dataset successfully simulated the described issues, including approximately 5% missing values in 'Age', potential outliers in 'Transaction\\_Amount' (maximum value significantly higher than the 75th percentile), and a highly imbalanced target variable where only 2% of samples were labeled as fraud.\n",
        "*   Median imputation was successfully applied to address the missing values in the 'Age' column, resulting in no remaining missing values in this column.\n",
        "*   Winsorizing at the 99th percentile was effective in handling outliers in the 'Transaction\\_Amount' column, significantly reducing the range of values compared to the original distribution.\n",
        "*   One-hot encoding was successfully applied to the 'Payment\\_Method' categorical variable, creating new binary columns for each payment method and removing the original categorical column.\n",
        "*   The SMOTE oversampling technique effectively balanced the target variable distribution, resulting in an equal number of samples for both fraud and non-fraud classes (9800 each).\n",
        "*   Numerical features ('Age', 'Transaction\\_Amount', and 'Transaction\\_Amount\\_Winsorized') were successfully scaled using `StandardScaler`, transforming their values to have a mean of 0 and a standard deviation of 1.\n",
        "*   The prepared data was successfully split into training and testing sets, with 80% allocated for training and 20% for testing.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The data is now prepared for training a machine learning model to detect fraudulent transactions. Given the balanced dataset and scaled numerical features, various classification algorithms can be applied.\n",
        "*   Further model evaluation should focus on appropriate metrics for imbalanced datasets, such as precision, recall, F1-score, and the Area Under the Receiver Operating Characteristic curve (AUC-ROC), rather than just accuracy.\n"
      ]
    }
  ]
}